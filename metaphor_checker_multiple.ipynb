{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7284c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0558cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_trf') #transformer model\n",
    "nlp1 = spacy.load('en_core_web_lg') #less accuracy, but does not use CUDA or require GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a9574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "pmatcher = PhraseMatcher(nlp1.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea25e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP nltk\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "stop_words = list(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c7de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP word embeddings\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import gensim.downloader as api\n",
    "dataset = api.load('text8')\n",
    "wv = Word2Vec(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e2a8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine computation\n",
    "import scipy as sp\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f539c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining useful functions/classes\n",
    "def clean_synonyms(synsets):\n",
    "    cleaned_synonyms = []\n",
    "    no_stop_words = []\n",
    "    syn_in_wv = []\n",
    "\n",
    "    \n",
    "    sep = '.'\n",
    "    for s in synsets:\n",
    "        synonym = s.name()\n",
    "        cleaned_synonyms.append(synonym.split(sep, 1)[0])\n",
    "    \n",
    "    for word in cleaned_synonyms:\n",
    "        if word not in stop_words:\n",
    "            no_stop_words.append(word)\n",
    "        if word in wv.wv.index_to_key:\n",
    "            syn_in_wv.append(word)\n",
    "    \n",
    "    return syn_in_wv\n",
    "\n",
    "class Sentence:\n",
    "    \n",
    "    def __init__(self, text = ''):\n",
    "        \n",
    "        self.text = text\n",
    "        \n",
    "        lower = text.lower()\n",
    "        \n",
    "        self.lower = lower\n",
    "        \n",
    "        \n",
    "        doc = nlp(lower)\n",
    "        \n",
    "        target = []\n",
    "        context = []\n",
    "                \n",
    "        for token in doc:\n",
    "            if token.pos_ == 'VERB':\n",
    "                if token.text in wv.wv.index_to_key:\n",
    "                    target.append(token.text)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                if token.text in wv.wv.index_to_key:\n",
    "                    context.append(token.text)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "        self.target = target\n",
    "        self.context = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4d08c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metaphor_checker2(sentence):\n",
    "\n",
    "    lowered_sentence = sentence.lower()\n",
    "    doc = nlp1(lowered_sentence)\n",
    "\n",
    "    targets = []\n",
    "    target_docs = []\n",
    "    context = []\n",
    "    contexts = []\n",
    "    target_vecs = []\n",
    "    chopped = sentence.lower().split(' ')\n",
    "    all_candidates = []\n",
    "    syn_vecs = []\n",
    "    context_vec_means = []\n",
    "    results = []\n",
    "                \n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            if token.text in wv.wv.index_to_key:\n",
    "                targets.append(token.text)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            if token.text in wv.wv.index_to_key:\n",
    "                context.append(token.text)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    for t in targets:\n",
    "        target_docs.append(nlp1(t))\n",
    "\n",
    "    #pmatcher.remove('TARGETS')\n",
    "    pmatcher.add('TARGETS', target_docs)\n",
    "\n",
    "    matches = pmatcher(doc)\n",
    "\n",
    "    for t in targets:\n",
    "        if len(t) == 1:\n",
    "            dummy = sentence.lower().split(' ')\n",
    "            i = chopped.index(t)\n",
    "            del dummy[i]\n",
    "            contexts.append(dummy)\n",
    "        else:\n",
    "            i = targets.index(t)\n",
    "            dummy = sentence.lower().split(' ')\n",
    "            match = matches[i]\n",
    "            start = match[1]\n",
    "            end = match[2]\n",
    "            del dummy[start: end]\n",
    "            contexts.append(dummy)\n",
    "\n",
    "    for i in range(0, len(targets)):\n",
    "        target_word = targets[i]\n",
    "        \n",
    "        direct_syn = [] #list of synsets for one target word\n",
    "        direct_syn.append(wn.synsets(target_word, pos = 'v'))\n",
    "        \n",
    "        for s in range(0, len(direct_syn)):\n",
    "            candidate_words = []\n",
    "            syn = direct_syn[s] #getting list of synsets for target\n",
    "            for word in syn: #for each synset in the list of synsets\n",
    "                lemmas = word.lemmas()\n",
    "                for l in lemmas:\n",
    "                    hyponyms = l.synset().hyponyms()\n",
    "                    if len(hyponyms) > 0:\n",
    "                        for h in hyponyms:\n",
    "                            sep = '.'\n",
    "                            name = h.name().split(sep, 1)[0]\n",
    "                            if name in wv.wv.index_to_key:\n",
    "                                if candidate_words.count(name) == 0:\n",
    "                                    candidate_words.append(name)\n",
    "                    else:\n",
    "                        name = l.name().split(sep, 1)[0]\n",
    "                        if name in wv.wv.index_to_key:\n",
    "                            if candidate_words.count(name) == 0:\n",
    "                                candidate_words.append(name)\n",
    "        all_candidates.append(candidate_words)\n",
    "    \n",
    "    for ac in all_candidates:\n",
    "        syn_vec = []\n",
    "        if len(ac) > 0:\n",
    "            for c in ac:\n",
    "                if c in wv.wv.index_to_key:\n",
    "                    syn_vec.append(wv.wv[c])\n",
    "            syn_vecs.append(syn_vec)\n",
    "\n",
    "    for c in contexts:\n",
    "        c_vec_list = []\n",
    "        for word in context:\n",
    "            c_vec_list.append(wv.wv[word])\n",
    "        context_mean = np.mean(c_vec_list, axis = 0)\n",
    "        context_vec_means.append(context_mean)\n",
    "    \n",
    "    for i in range(0, len(targets)):\n",
    "        target = targets[i]\n",
    "        target_syns = syn_vecs[i]\n",
    "        target_context = context_vec_means[i]\n",
    "    \n",
    "        comparison = []\n",
    "        for s in target_syns:\n",
    "            comparison.append(distance.cosine(s, target_context))\n",
    "        \n",
    "        comparison_array = np.array(comparison)\n",
    "            \n",
    "        best_fit_index = comparison.index(np.amax(comparison_array))\n",
    "        best_fit_word = all_candidates[i][best_fit_index]\n",
    "                    \n",
    "        similarity = wv.wv.similarity(best_fit_word, target)\n",
    "        \n",
    "        if similarity > 0.45: #Different threshold?\n",
    "            results.append([target, 'LITERAL', similarity, best_fit_word])\n",
    "        else:\n",
    "            results.append([target, 'METAPHORICAL', similarity, best_fit_word])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45427e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ate', 'LITERAL', 0.4579959, 'forage'],\n",
       " ['devoured', 'METAPHORICAL', 0.09248138, 'consume']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'She ate the apple and devoured the novel.' #Expect: literal, metaphorical\n",
    "metaphor_checker2(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d246ff02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['flew', 'METAPHORICAL', 0.34615675, 'airlift'],\n",
       " ['sang', 'METAPHORICAL', 0.2641522, 'madrigal']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = 'The bird flew like the wind and sang like an angel.' #Expect: metaphorical, literal\n",
    "metaphor_checker2(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "111f5b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rang', 'LITERAL', 0.5495239, 'enclose'],\n",
       " ['struck', 'METAPHORICAL', 0.032689016, 'assume']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2 = 'The boy rang the bell and the clock struck noon.' #Expect: literal, metaphorical\n",
    "metaphor_checker2(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cfda13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:openai] *",
   "language": "python",
   "name": "conda-env-openai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
