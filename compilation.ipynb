{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f67179",
   "metadata": {},
   "source": [
    "# Metaphor identification algorithms\n",
    "## from Neuman et al. 2013\n",
    "\n",
    "Main concern/motivation: there is only one person (me) on this project, and I don't believe only me annotating is likely to be helpful/useful\n",
    "\n",
    "### Definitions (from Krishnakumaran and Zhu 2007)\n",
    "\n",
    "Type I metaphor: \"a subject noun is associated with an object noun via a form of the copula verb ‘to be’, such as in the case of ‘God is a king’.\"\n",
    "\n",
    "Type II metaphor: \"the verb is the focus of the metaphorical use representing the act of a subject noun on an object noun, such as in the case of ‘The war absorbed his energy.’\"\n",
    "\n",
    "Type III metaphor: \"involve an adjective-noun phrase such as ‘sweet girl’.\" \n",
    "\n",
    "### Detecting type II metaphors\n",
    "\n",
    "Phrase = < N1, V, N2 > where V represents the (metaphorical) action of N1 on N2\n",
    "\n",
    "1. Identify the 100 most concrete object nouns associated with the verb V in a corpus\n",
    "2. Categorize the 100 nouns by using WordNet\n",
    "3. Categorize the object noun N2\n",
    "\n",
    "Layer 1:\n",
    "4. If none of the object noun categories overlaps with one of the categories of the 100 nouns associated with the verb, then return METAPHORICAL\n",
    "\n",
    "Layer 2:\n",
    "5. Find the main category of the object noun using ConceptNet\n",
    "6. If the main category is not included in the categories of the 100 nouns, then return METAPHORICAL; else return LITERAL\n",
    "\n",
    "Above algorithm depends on concept called \"mutual information\" => have you come across this before? \n",
    "def: \"a quantity that measures the mutual dependence of two random variables\"\n",
    "\n",
    "### Detecting type I metaphors\n",
    "**Still reasoning this one out => manually annotating White House sample didn't yield many examples of type I metaphors\n",
    "\n",
    "Phrase = < N1, N2 > where N1 is the subject noun (\"God\") and N2 is the object noun (\"king\")\n",
    "\n",
    "1. Identify the categories of N1 and N2 >>> if they do not overlap, then return METAPHORICAL\n",
    "\n",
    "Layer 1: \n",
    "2. Find main categories of N1 and N2 using ConceptNet\n",
    "3. If the 2 main categories are different, then return METAPHORICAL\n",
    "\n",
    "Layer 2 (disambiguating instances where N1 and N2 are from the same category, e.g., \"my CAT is a TIGER\"):\n",
    "4. Identify the 100 nouns associated with N1 and N2 separately\n",
    "5. Categorize the 100 nouns associated with N1 and N2 separately\n",
    "6. If none of the nouns' categories overlap with each other, then return METAPHORICAL; else return LITERAL\n",
    "\n",
    "Workflow: pass texts and source domain, identify instances there was potential metaphor usage\n",
    "> Weakness: still need a person to build larger frames\n",
    "\n",
    "Question: are there methods for identifying these larger frames (ML)? In your view, would it be possible?\n",
    "> Do you have any suggestions for overcoming these challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb692813",
   "metadata": {},
   "source": [
    "Another question: stemmer/lemmatizer/tokenizer included in NLTK >>> how come you don't use them?\n",
    "Please give me some advice on best practices when it comes to corpus-based work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "#Libraries for Webscraping\n",
    "import requests\n",
    "\n",
    "#Data preparation\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Visualization\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c80e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing previously written functions\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from notebooks.functions import GetLinks, CookedSoup, CleanText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2306d5",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88622de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function to get url links for scraping\n",
    "def GetLinks(url):\n",
    "    \n",
    "    #Making broth\n",
    "    html_text = requests.get(url)\n",
    "    broth = BeautifulSoup(html_text.text, 'lxml')\n",
    "    urlset = broth.find_all('a', class_='news-item__title')\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    for i in np.arange(0, len(urlset)):\n",
    "        url = urlset[i]\n",
    "        links.append(url.get('href'))\n",
    "        \n",
    "    return links\n",
    "\n",
    "#Creating function to clean collected text data\n",
    "class CleanText:\n",
    "    def __init__(self, text = ''):\n",
    "        self.text = text\n",
    "        \n",
    "         #Cleaning added text\n",
    "        # obvious-ly => obvious ly (lemmatization)\n",
    "        words = ''.join((filter(lambda x: x in string.printable, text)))\n",
    "        words = words.replace('—', '')\n",
    "        table = str.maketrans('', '', string.punctuation + '’‘' + '\"\"' + '–' + '­')\n",
    "        words = [w.translate(table).lower() for w in words.split()]\n",
    "        stop_words = stopwords.words('english')\n",
    "        words = list(filter(lambda w: w not in stop_words, words))\n",
    "        words = list(filter(lambda w: w.isalpha(), words))\n",
    "        \n",
    "        self.words = words\n",
    "        \n",
    "#Creating function => pass list of links, give dict of scraped data\n",
    "\n",
    "def CookedSoup(html_links):\n",
    "    \n",
    "    titles = []\n",
    "    time = []\n",
    "    btype = []\n",
    "    original_text = []\n",
    "    cleaned_words = []\n",
    "    \n",
    "    counter = np.arange(0,len(html_links))\n",
    "\n",
    "    #Making soup of link\n",
    "    for i in counter:\n",
    "        html = requests.get(html_links[i])\n",
    "        soup = BeautifulSoup(html.text, 'lxml')\n",
    "            \n",
    "    #Getting Titles\n",
    "        title_html = soup.find('h1', class_='page-title topper__title news')\n",
    "        \n",
    "        titles.append(title_html.text.strip('\\n').strip('\\t').replace('\\xa0', ' '))\n",
    "                \n",
    "    #Getting meta-data\n",
    "        time_html = soup.find('time', class_='posted-on entry-date published updated')\n",
    "        time_str = time_html.text\n",
    "        datetime_object = datetime.strptime(time_str, '%B %d, %Y')\n",
    "        time.append(datetime_object)\n",
    "        \n",
    "        btype_html = soup.find('a', rel= 'category tag')\n",
    "        btype.append(btype_html.text)\n",
    "        \n",
    "    #Getting text content\n",
    "        briefing_text = []\n",
    "        \n",
    "        for para in soup.find_all('section', class_ = 'body-content'):\n",
    "            text_range = np.arange(5, len(soup.find_all('p')[5:])-1)\n",
    "            \n",
    "        briefing_text.append(para.text.strip('\\n').strip('\\t').\n",
    "                             replace('\\xa0', ' ').replace('\\n', ''))\n",
    "        \n",
    "        original_text.append(briefing_text)\n",
    "                        \n",
    "    #Cleaning text content\n",
    "        \n",
    "        cleaned_text = CleanText(briefing_text[0])\n",
    "        cleaned_words.append(cleaned_text.words)\n",
    "        \n",
    "    #And now we loop\n",
    "        i = i+1\n",
    "        \n",
    "    #Creating dict object\n",
    "    \n",
    "    briefing_info = {'title': titles, 'url':html_links, 'time': time, 'btype': btype, \n",
    "                     'original_text': original_text, 'cleaned_words': cleaned_words}\n",
    "\n",
    "    return briefing_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae91c62",
   "metadata": {},
   "source": [
    "# Scraping website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3068cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing to scrape entire website\n",
    "\n",
    "#Making soup of initial website\n",
    "url = \"https://www.whitehouse.gov/briefing-room/press-briefings/\"\n",
    "\n",
    "html_text = requests.get(url)\n",
    "soup = BeautifulSoup(html_text.text, 'lxml')\n",
    "\n",
    "#Creating indices\n",
    "max_page = int(soup.find_all('a', class_='page-numbers')[-1].text.replace('Page ', ''))+1\n",
    "\n",
    "pages_list = np.arange(2, max_page)\n",
    "#type(pages_list)\n",
    "\n",
    "#Creating page url list\n",
    "blank_url = 'https://www.whitehouse.gov/briefing-room/press-briefings/{page_number}'\n",
    "page_number = 'page/{number}/'\n",
    "page1_url = 'https://www.whitehouse.gov/briefing-room/press-briefings/'\n",
    "\n",
    "urls = []\n",
    "\n",
    "for page in pages_list:\n",
    "    page_url = blank_url.format(page_number = page_number.format(number = str(page)))\n",
    "    \n",
    "    urls.append(page_url)\n",
    "    \n",
    "    page = page + 1\n",
    "    \n",
    "urls.insert(0, page1_url)\n",
    "\n",
    "#print(urls)\n",
    "\n",
    "#Creating WH_briefing_df\n",
    "column_list = ['title', 'time', 'btype','original_text', 'cleaned_words']\n",
    "\n",
    "WH_briefing_df = pd.DataFrame(columns = column_list)\n",
    "\n",
    "#Filling WH_briefing_dict\n",
    "\n",
    "dict_range = np.arange(0, max_page-1)\n",
    "\n",
    "for i in dict_range:\n",
    "    \n",
    "    links = GetLinks(urls[i])\n",
    "    page_dict = CookedSoup(links)\n",
    "    page_df = pd.DataFrame(data = page_dict)\n",
    "    \n",
    "    WH_briefing_df = WH_briefing_df.append(page_df)\n",
    "    \n",
    "    #print(len(WH_briefing_df['titles'].index))\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "press_briefings = WH_briefing_df.reset_index(drop = True)\n",
    "press_briefings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424bc1e",
   "metadata": {},
   "source": [
    "# Creating CSV of scraped results\n",
    "## Do not run w/o modifying document name!\n",
    "\n",
    "from pathlib import Path  \n",
    "filepath = Path('/Users/daisyliu/Desktop/Research/UTEA_covid19_metaphors/notebooks/wordclouds/data/press_briefings.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "press_briefings.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "\n",
    "def WordCloudCreator(df):\n",
    "    \n",
    "    counter = int(len(df.index))\n",
    "    \n",
    "    for i in np.arange(0, counter-1):\n",
    "        WordCloud_Data = df.iloc[i]\n",
    "        \n",
    "        plt_title = str(i)\n",
    "        plt_data = WordCloud_Data['cleaned_words']\n",
    "        \n",
    "        d = {}\n",
    "        \n",
    "        for word in plt_data:\n",
    "            if word not in d.keys():\n",
    "                d[word] = 1\n",
    "            else:\n",
    "                d[word] = int(d[word]) + 1\n",
    "        \n",
    "        wordcloud = WordCloud(background_color = 'white', max_words = 50).generate_from_frequencies(d)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        plt.savefig(plt_title, format = 'png', bbox_inches = 'tight')\n",
    "        \n",
    "        \n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577ce6d",
   "metadata": {},
   "source": [
    "# Generating separate dataframes\n",
    " \n",
    "## Colour Coding Legend\n",
    "Keywords: 'vaccin-,' 'mask-,' 'pandemic,' 'covid-', 'virus'\n",
    "\n",
    "Note: wild card '*' replaced with '-'\n",
    "\n",
    "### Green\n",
    "Green files produce WordCloud that includes one or more of the keywords (i.e., highly relevant)\n",
    "\n",
    "### Red\n",
    "Red files produce WordClouds that do not include any of the keywords, 'cdc,' 'fda,' 'health' (i.e., irrelevant)\n",
    "\n",
    "### Orange\n",
    "Orange files produce WordClouds that might be from relevant articles\n",
    "\n",
    "### Yellow\n",
    "Yellow files produce WordClouds that include words such as 'health' (i.e., partly relevant)\n",
    "\n",
    "### Blue\n",
    "Blue files produce nonsensical WordClouds—need to clean again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DFs of green files only\n",
    "\n",
    "green_briefings = pd.DataFrame(columns = column_list)\n",
    "\n",
    "green_path = '/Users/daisyliu/Desktop/Research/UTEA_covid19_metaphors/notebooks/wordclouds/green'\n",
    "\n",
    "green_files = [f for f in listdir(green_path) if isfile(join(green_path, f))]\n",
    "\n",
    "for file in green_files:\n",
    "    i = int(file)\n",
    "    #print(i)\n",
    "    green_briefings = green_briefings.append(press_briefings.iloc[i])\n",
    "\n",
    "green_briefings['file_number'] = green_files\n",
    "green_briefings.reset_index(drop = True)\n",
    "green_briefings = green_briefings.append(press_briefings.iloc[28])\n",
    "green_briefings.at[28, 'file_number'] = \"28\"\n",
    "green_briefings = green_briefings.reset_index()\n",
    "\n",
    "green_briefings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc58988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating DFs of red files only\n",
    "\n",
    "red_briefings = pd.DataFrame(columns = column_list)\n",
    "\n",
    "red_path = '/Users/daisyliu/Desktop/Research/UTEA_covid19_metaphors/notebooks/wordclouds/red'\n",
    "\n",
    "red_files = [f for f in listdir(red_path) if isfile(join(red_path, f))]\n",
    "\n",
    "red_files.remove('.DS_Store')\n",
    "\n",
    "for file in red_files:\n",
    "    i = int(file)\n",
    "    #print(i)\n",
    "    red_briefings = red_briefings.append(press_briefings.iloc[i])\n",
    "\n",
    "red_briefings['file_number'] = red_files\n",
    "red_briefings.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DFs of orange files only\n",
    "\n",
    "orange_briefings = pd.DataFrame(columns = column_list)\n",
    "\n",
    "orange_path = '/Users/daisyliu/Desktop/Research/UTEA_covid19_metaphors/notebooks/wordclouds/orange'\n",
    "\n",
    "orange_files = [f for f in listdir(orange_path) if isfile(join(orange_path, f))]\n",
    "\n",
    "for file in orange_files:\n",
    "    i = int(file)\n",
    "    #print(i)\n",
    "    orange_briefings = orange_briefings.append(press_briefings.iloc[i])\n",
    "\n",
    "orange_briefings['file_number'] = orange_files\n",
    "orange_briefings.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e94502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DFs of yellow files only\n",
    "\n",
    "yellow_briefings = pd.DataFrame(columns = column_list)\n",
    "\n",
    "yellow_path = '/Users/daisyliu/Desktop/Research/UTEA_covid19_metaphors/notebooks/wordclouds/yellow'\n",
    "\n",
    "yellow_files = [f for f in listdir(yellow_path) if isfile(join(yellow_path, f))]\n",
    "\n",
    "for file in yellow_files:\n",
    "    i = int(file)\n",
    "    #print(i)\n",
    "    yellow_briefings = yellow_briefings.append(press_briefings.iloc[i])\n",
    "\n",
    "yellow_briefings['file_number'] = yellow_files\n",
    "yellow_briefings = yellow_briefings.reset_index(drop = True)\n",
    "\n",
    "yellow_briefings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16012890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DFs of blue files only\n",
    "\n",
    "blue_briefings = pd.DataFrame(columns = column_list)\n",
    "\n",
    "blue_path = '/Users/daisyliu/Desktop/Research/UTEA_covid19_metaphors/notebooks/wordclouds/blue'\n",
    "\n",
    "blue_files = [f for f in listdir(blue_path) if isfile(join(blue_path, f))]\n",
    "\n",
    "for file in blue_files:\n",
    "    i = int(file)\n",
    "    #print(i)\n",
    "    blue_briefings = blue_briefings.append(press_briefings.iloc[i])\n",
    "\n",
    "blue_briefings['file_number'] = blue_files\n",
    "blue_briefings.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c519c1c4",
   "metadata": {},
   "source": [
    "# Creating corpus text files of entries from green_briefings df\n",
    "\n",
    "## DO NOT RUN UNLESS YOU WANT ANOTHER SAMPLE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872061fc",
   "metadata": {},
   "source": [
    "green_index = [] #Creating number strings\n",
    "\n",
    "for n in green_briefings.index:\n",
    "    green_index.append(str(n))\n",
    "    \n",
    "### Creating corpus header\n",
    "file_name = '{name}'\n",
    "article_name = '<{article_name}>'\n",
    "btype = '<{btype}>'\n",
    "time = '<{time}>'\n",
    "url = '<{url}>'\n",
    "    \n",
    "for row in green_briefings.index:\n",
    "    \n",
    "    data = green_briefings.iloc[row]\n",
    "    #print(data)\n",
    "    \n",
    "    row_file_name = file_name.format(name = str(data.file_number))+\".txt\"\n",
    "    \n",
    "    with open(row_file_name, \"a\") as file:\n",
    "        file.write(article_name.format(article_name = data.title)+'\\n')\n",
    "        file.write(btype.format(btype = data.btype)+'\\n')\n",
    "        file.write(time.format(time = data.time)+'\\n')\n",
    "        file.write(url.format(url = data.url)+'\\n')\n",
    "        file.write(data.original_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d6266",
   "metadata": {},
   "source": [
    "### Now generating sample\n",
    "\n",
    "sample = random.sample(range(0, 175), 87)\n",
    "\n",
    "for s in sample:\n",
    "    with open(\"sample.txt\", \"a\") as file:\n",
    "        file.write(green_briefings.iloc[s].original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17147fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
